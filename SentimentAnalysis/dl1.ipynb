{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 1\n",
    "\n",
    "In dit practicum gaan we het [Keras](https://keras.io/) deep learning framework voor Python gebruiken. De documentatie omschrijft Keras als volgt:\n",
    "\n",
    ">Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Het ontwikkelen en testen van neurale netwerken kan een vermoeiend proces zijn. Keras levert een intuïtieve API voor het trainen, testen en deployen van neurale netwerken zonder dat we ons moeten bekommeren om technische details. Om te beginnen, importeren we Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras kan gebruikt worden met verschillende *backends*. Een backend is een ander framework dat de low-level details van de implementatie afhandelt, zoals GPU optimalisatie en gedistribueerde berekeningen. Op dit moment ondersteunt Keras volgende backends: [TensorFlow](https://www.tensorflow.org/), [Theano](http://deeplearning.net/software/theano/) en [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/). Wanneer we Keras importeren, laat het ons weten welk backend het gebruikt.\n",
    "\n",
    "## Deep Neural Network basics\n",
    "\n",
    "Een diep neuraal network (DNN) bestaat uit verschillende lagen:\n",
    "\n",
    "$$\n",
    "    f(x) = g_L(W_Lg_{L-1}(\\dots g_1(W_1x + b_1) \\dots) + b_L)\n",
    "$$\n",
    "\n",
    "Hier zijn $W_1, \\dots, W_L$ matrices (de *gewichten* van het netwerk), $b_1, \\dots, b_L$ zijn vectoren (de *biases*) en $g_1, \\dots, g_L$ zijn de *activatiefuncties*. We kunnen er een tekening bij maken:\n",
    "\n",
    "![Een neuraal netwerk](images/mlp.png)\n",
    "\n",
    "Elke top in dit netwerk stelt een inwendig product voor van de gewichtsvector met de input, plus een bias term. Dit resultaat gaat dan door een niet-lineaire activatiefunctie. Voorbeelden van typische activatiefuncties zijn\n",
    "\n",
    "1. de logistic sigmoid:\n",
    "$$\n",
    "    \\mathrm{sigmoid}(z) = \\frac{1}{1 + \\exp(-z)},\n",
    "$$\n",
    "\n",
    "2. de rectified linear unit (RELU):\n",
    "$$\n",
    "    \\mathrm{relu}(z) = \\max(0,z),\n",
    "$$\n",
    "\n",
    "3. de scaled exponential linear unit (SELU):\n",
    "$$\n",
    "    \\mathrm{selu}(z) = \\lambda\\left\\{\\begin{matrix}\n",
    "        z, & \\mbox{als $z > 0$}\\\\\n",
    "        \\alpha(\\exp(z)-1), & \\mbox{als $z \\leq 0$}\n",
    "    \\end{matrix}\\right..\n",
    "$$\n",
    "Hier zijn $\\lambda$ en $\\alpha$ hyperparameters die vastliggen voordat het netwerk wordt getraind.\n",
    "\n",
    "4. de softmax:\n",
    "$$\n",
    "    \\mathrm{softmax}(z) = \\frac{\\exp(z)}{\\sum_i\\exp(z_i)}.\n",
    "$$\n",
    "De softmax wordt doorgaans gebruikt in de laatste laag van het netwerk. Het heeft de eigenschap dat voor elke $z \\in \\mathbb{R}^q$,\n",
    "$$\\begin{aligned}\n",
    "    \\mathrm{softmax}(z) &\\in [0,1]^q, & \\sum_{i=1}^q\\mathrm{softmax}(z)_i &= 1.\n",
    "\\end{aligned}$$\n",
    "Met andere woorden, de output van $\\mathrm{softmax}(z)$ kan men interpreteren als een kansendistributie over $q$ mogelijkheden.\n",
    "\n",
    "We maken nu een simpel neuraal netwerk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=64, activation='relu', input_dim=30),\n",
    "    Dense(units=10, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit model bestaat uit drie *dense* of *fully-connected* lagen. Dit zijn de \"klassieke\" lagen die gewoon een lineaire transformatie uitvoeren gevolgd door een activatiefunctie. In ons geval wordt dit netwerk formeel gegeven door\n",
    "\n",
    "$$\n",
    "    f: \\mathbb{R}^{30} \\to \\mathbb{R}^{2}: x \\mapsto \\mathrm{softmax}(W_3\\mathrm{relu}(W_2\\mathrm{relu}(W_1x + b_1) + b_2) + b_3).\n",
    "$$\n",
    "\n",
    "Verder geldt $W_1 \\in \\mathbb{R}^{64 \\times 30}$, $W_2 \\in \\mathbb{R}^{10 \\times 64}$, $W_3 \\in \\mathbb{R}^{2 \\times 10}$, $b_1 \\in \\mathbb{R}^{64}$, $b_2 \\in \\mathbb{R}^{10}$ en $b_3 \\in \\mathbb{R}^2$. Het aantal parameters van dit model is dus\n",
    "\n",
    "$$\n",
    "    64 \\times 30 + 10 \\times 64 + 2 \\times 10 + 64 + 10 + 2 = 2656.\n",
    "$$\n",
    "\n",
    "Op dit moment hebben deze parameters natuurlijk nog geen nuttige waarden. Het zoeken naar waarden voor de gewichten en biases zodanig dat het netwerk een bepaalde prestatiemaat maximaliseert is het doel van een *leeralgoritme*.\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "In de supervised learning setting krijgen we een dataset van gelabelde observaties,\n",
    "\n",
    "$$\n",
    "    D = \\{ (x_i,y_i) \\in \\mathbb{R}^d \\times \\{1, \\dots, C\\} \\mid i = 1, \\dots, N \\}.\n",
    "$$\n",
    "\n",
    "We veronderstellen dat deze observaties onafhankelijk zijn en gelijk verdeeld volgens een (onbekende) kansmaat $P$. Elk element $(x_i,y_i) \\in D$ bestaat uit een vector $x_i \\in \\mathbb{R}^d$ en een label $y_i \\in \\{1, \\dots, C\\}$. Wij willen nu een netwerk bouwen zodat\n",
    "\n",
    "$$\n",
    "    \\Pr_{(x,y) \\sim P}[f(x) = y] = 1.\n",
    "$$\n",
    "\n",
    "We kennen $P$ natuurlijk niet, anders zou er geen probleem zijn. We moeten dit dus indirect optimaliseren via een *empirische risicofunctie*\n",
    "\n",
    "$$\n",
    "    R_{\\mathrm{emp}}(f) = \\frac{1}{N}\\sum_{i=1}^N\\ell(f(x_i), y_i).\n",
    "$$\n",
    "\n",
    "Dit is een empirische schatting van het *verwacht risico*\n",
    "\n",
    "$$\n",
    "    R(f) = \\underset{(x,y) \\sim P}{\\mathbb{E}}[\\ell(f(x),y)],\n",
    "$$\n",
    "\n",
    "dat we niet exact kunnen bepalen. Hier is $\\ell(f(x),y)$ de *verliesfunctie* die meet hoe ernstig de uitvoer van ons netwerk afwijkt van de gewenste uitvoer. Voor classificatieproblemen kan men de 0/1 loss gebruiken,\n",
    "\n",
    "$$\n",
    "    \\ell(f(x),y) = \\left\\{\\begin{matrix}\n",
    "        1, & \\mbox{als $f(x) \\neq y$}\\\\\n",
    "        0, & \\mbox{als $f(x) = y$}\n",
    "    \\end{matrix}\\right..\n",
    "$$\n",
    "\n",
    "Voor andere problemen zoals regressie kan het zinvoller zijn om een verlies te gebruiken zoals\n",
    "\n",
    "$$\n",
    "    \\ell(f(x),y) = \\|f(x)-y\\|_2^2.\n",
    "$$\n",
    "\n",
    "We lossen nu volgend probleem op:\n",
    "\n",
    "$$\n",
    "    \\min_f R_{\\mathrm{emp}}(f).\n",
    "$$\n",
    "\n",
    "In de praktijk komt het oplossen van dit probleem neer op onze gewichten en biases aanpassen totdat we geen reductie meer krijgen van het empirisch risico. Er bestaan veel optimalisatie-algoritmen die dit probleem kunnen oplossen, maar we gaan hier niet in op details (geïnteresseerde studenten kunnen altijd het vak *Optimalisatietechnieken* volgen). Het belangrijkste is dat deze algoritmen altijd iteratief werken op *mini-batches* van de trainingdata, nooit op de volledige dataset tenzij die heel klein is. Het aantal minibatches wordt bepaald door de *batchgrootte*. Op een hoog niveau verloopt de optimalisatie dus als volgt:\n",
    "\n",
    "1. Splits de trainingdata op in mini-batches met aantal samples gelijk aan de batchgrootte.\n",
    "2. Voor elke minibatch, bereken updates aan de gewichten en biases die uitsluitend op de minibatch gebaseerd zijn.\n",
    "3. Herhaal stap 2 tot convergentie.\n",
    "\n",
    "Elke uitvoering van de lus in stap 2 noemt men een *epoch*. Je zal altijd het aantal epochs en de batchgrootte moeten opgeven als je een neuraal netwerk traint. Zorg er ook voor dat de batchgrootte een gehele deler is van de grootte van de dataset, anders zullen sommige algoritmen gewoon niet willen uitvoeren. Typische batchgroottes zijn kleine machten van 2 zoals 64 of 128. Mocht de grootte van de dataset priem zijn, zal je moeten subsamplen.\n",
    "\n",
    "Om ons model te trainen in Keras, compileren we het met een verliesfunctie en een optimalisatie-algoritme dat dit verlies zal trachten te minimaliseren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat het `metrics` argument een lijst nodig heeft, omdat we meerdere metrieken kunnen berekenen per model. Zie [de documentatie](https://keras.io/metrics/) voor een volledige lijst van ondersteunde metrieken. We kunnen het model nu fitten op een dataset, zoals de Wisconsin breast cancer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "wisconsin = load_breast_cancer()\n",
    "x_data = wisconsin['data']\n",
    "y_data = wisconsin['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bekijken eerst wat statistieken over deze dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Aantal samples: {}'.format(x_data.shape[0]))\n",
    "print('Aantal features: {}'.format(x_data.shape[1]))\n",
    "\n",
    "class0 = x_data[y_data == 0].shape[0]\n",
    "class1 = x_data[y_data == 1].shape[0]\n",
    "ratio = max([class0, class1]) / min([class0, class1])\n",
    "print('Klasse 0: {}'.format(class0))\n",
    "print('Klasse 1: {}'.format(class1))\n",
    "print('Imbalance ratio: {}'.format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De klassen bevatten respectievelijk 212 (borstkanker) en 357 (geen borstkanker) samples. De meerderheidsklasse is dus ongeveer 1.68x zo groot als de minderheidsklasse; we gaan hier later rekening mee moeten houden. Het netwerk verwacht dat de labels vectoren zijn van dimensie 2, dus voeren we *one-hot encoding* uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array([[1., 0.] if y == 0 else [0., 1.] for y in y_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu shufflen en splitsen we de data in training en test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_data, y_data = shuffle(x_data, y_data)\n",
    "\n",
    "p = .8\n",
    "idx = int(x_data.shape[0] * p)\n",
    "x_train, y_train = x_data[:idx], y_data[:idx]\n",
    "x_test, y_test = x_data[idx:], y_data[idx:]\n",
    "\n",
    "x_mean, x_std = x_train.mean(), x_train.std()\n",
    "x_train -= x_mean\n",
    "x_train /= x_std\n",
    "\n",
    "x_test -= x_mean\n",
    "x_test /= x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat we de datasets genormaliseerd hebben zodat het gemiddelde 0 is en de variantie 1. Keras kan de parameters nu proberen schatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omwille van willekeur in de initialisatie van de parameters alsook het algoritme zelf, kan het zijn dat verschillende uitvoeringen van de `fit` methode andere resultaten geven. Je zou een accuracy van minstens 90% moeten bereiken na een handvol uitvoeringen.\n",
    "\n",
    "Nu het model getraind is, kunnen we er voorspellingen mee doen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.argmax(model.predict(x_test, batch_size=65), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met deze array kunnen we de accuracy zelf berekenen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(np.equal(classes, np.argmax(y_test, axis=1)))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras kan dit ook voor ons doen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = model.evaluate(x_test, y_test, batch_size=65)\n",
    "print('Loss: {}'.format(evals[0]))\n",
    "print('Accuracy: {}'.format(evals[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De klassen van deze dataset zijn niet gebalanceerd, dus we zouden ook best naar de balanced accuracy kijken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(y_test, axis=1)\n",
    "idx0 = (labels == 0)\n",
    "idx1 = (labels == 1)\n",
    "\n",
    "acc0 = np.mean(np.equal(classes[idx0], labels[idx0]))\n",
    "acc1 = np.mean(np.equal(classes[idx1], labels[idx1]))\n",
    "bal_acc = (acc0 + acc1) / 2\n",
    "print('Balanced accuracy: {}'.format(bal_acc))\n",
    "print('\\tKlasse 0: {}'.format(acc0))\n",
    "print('\\tKlasse 1: {}'.format(acc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaal gezien zou er een merkbaar verschil moeten zijn tussen klasse 0 (de minderheid) en klasse 1 (de meerderheid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 1\n",
    "\n",
    "Wat gebeurt er als je geen normalisatie van de data doet voordat je het netwerk traint? Kan je dit verklaren? Waarom doen we de moeite van de normalisatie apart uit te voeren op de training en test data en enkel met de statistieken berekend op de training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 2\n",
    "\n",
    "Keras ondersteunt een heel aantal verliesfuncties (zie [de documentatie](https://keras.io/losses/)). Welke zou je kiezen voor welke taak en waarom? Waarom staat de 0/1 loss die hierboven werd beschreven zelfs niet eens in deze lijst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 3\n",
    "\n",
    "Keras heeft ook een aantal activatiefuncties naast de RELU (zie [de documentatie](https://keras.io/activations/)). Experimenteer met verschillende keuzes op de Wisconsin dataset. Probeer te verklaren wat je observeert. **Hint:** visualiseer de activatiefuncties eens met een plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 4\n",
    "\n",
    "Waarom maakt het uit dat de klassen niet gebalanceerd zijn? Kan je technieken bedenken om de klassen gebalanceerd te maken?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 5\n",
    "\n",
    "Keras heeft een aantal [ingebouwde datasets](https://keras.io/datasets/) waar je mee kunt experimenteren. Probeer eens een neuraal netwerk te trainen dat goed presteert op de Boston housing price regression data set. Merk op dat dit een *regressieprobleem* is, geen *classificatieprobleem* zoals we hiervoor beschouwd hebben. Je zal dus een lichtjes andere aanpak nodig hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}